# -*- coding: utf-8 -*-
"""Distillation Column Ethanol Purity Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H1jePlxTOnN_0iJOhp-rX-6LCeVDUxIg

### STEP 0: IMPORT DEPENDENDENT LIBRARIES
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

"""### STEP 1: LOAD AND EXPLORE DATA"""

# 1. Load Dataset
df = pd.read_csv('dataset_distill.csv', sep=';')

# 2. Convert All Temperatures (T1 to T14) from Kelvin to Celsius
temp_cols = [f'T{i}' for i in range(1, 15)]
df[temp_cols] = df[temp_cols] - 273.15  # Vectorized operation (Faster than loop)

# 3. Quick Health Check
print(f"‚úì Dataset Loaded: {df.shape}")
print(f"‚úì Temp Range (¬∞C): {df['T1'].min():.1f} - {df['T14'].max():.1f}")
print(f"‚úì Missing Values: {df.isnull().sum().sum()}")
print(f"‚úì Target Mean: {df['Ethanol concentration'].mean():.4f}")

# Display first 2 rows
display(df.head(2))

"""### STEP 2: DATA CLEANING"""

# 1. Standard Cleaning
initial_rows = len(df)
df_clean = df.drop_duplicates().dropna()

# 2. Ensure Target is Numeric
df_clean['Ethanol concentration'] = pd.to_numeric(df_clean['Ethanol concentration'], errors='coerce')
df_clean = df_clean.dropna(subset=['Ethanol concentration'])

# 3. Remove Extreme Outliers (Keep middle 98% of data)
lower_limit = df_clean['Ethanol concentration'].quantile(0.01)
upper_limit = df_clean['Ethanol concentration'].quantile(0.99)

df_clean = df_clean[
    (df_clean['Ethanol concentration'] >= lower_limit) &
    (df_clean['Ethanol concentration'] <= upper_limit)
]

print(f"‚úì Data Cleaned: {initial_rows} -> {len(df_clean)} rows (Removed {initial_rows - len(df_clean)})")

"""### STEP 3: FEATURE ENGINEERING"""

# 1. Ensure numeric types for flow rates
flow_cols = ['L', 'V', 'F', 'D', 'B']
df_clean[flow_cols] = df_clean[flow_cols].apply(pd.to_numeric, errors='coerce')
df_clean.dropna(subset=flow_cols, inplace=True)

# 2. Define Key Temperatures
# FIX: Renamed 'Temp_Bottom/Reboiler_Temp' to just 'Temp_Bottom' for consistency
df_clean['Temp_Top'] = df_clean['T1']
df_clean['Temp_Bottom'] = df_clean['T14']

# This Diff is critical for the "Interaction Terms"
df_clean['Temp_Diff_Global'] = df_clean['Temp_Bottom'] - df_clean['Temp_Top']

# 3. Create Operating Ratios (Safe Division)
epsilon = 1e-6
df_clean['Reflux_Ratio'] = df_clean['L'] / (df_clean['V'] + epsilon)
df_clean['Reboiler_Intensity'] = df_clean['V'] / (df_clean['F'] + epsilon)
df_clean['Condenser_Load'] = df_clean['L'] / (df_clean['F'] + epsilon)
df_clean['Distillate_Withdrawal'] = df_clean['D'] / (df_clean['F'] + epsilon)
df_clean['Bottoms_Withdrawal'] = df_clean['B'] / (df_clean['F'] + epsilon)
df_clean['Column_Load'] = (df_clean['L'] + df_clean['V']) / (df_clean['F'] + epsilon)

# 4. Feed Normalization (CRITICAL STEP)
feed_mean = df_clean['F'].mean()
df_clean['Feed_Normalized'] = df_clean['F'] / feed_mean
print(f"‚ö†Ô∏è IMPORTANT FOR APP: Mean Feed Flow (F) is: {feed_mean:.4f}")

# 5. Create Interaction Terms (The "Secret Sauce")
df_clean['Reflux_x_Temp_Top'] = df_clean['Reflux_Ratio'] * df_clean['Temp_Top']
df_clean['Reflux_x_Temp_Diff'] = df_clean['Reflux_Ratio'] * df_clean['Temp_Diff_Global']
# FIX: Used corrected 'Temp_Bottom' name here
df_clean['Reboiler_x_Temp_Bottom'] = df_clean['Reboiler_Intensity'] * df_clean['Temp_Bottom']
df_clean['Feed_x_Reflux'] = df_clean['Feed_Normalized'] * df_clean['Reflux_Ratio']
df_clean['Feed_x_Reboiler'] = df_clean['Feed_Normalized'] * df_clean['Reboiler_Intensity']

# 6. Efficiency Metrics
df_clean['Separation_Duty'] = df_clean['Reflux_Ratio'] * df_clean['Reboiler_Intensity']
df_clean['Column_Efficiency'] = df_clean['Reflux_Ratio'] * df_clean['Column_Load']

print(f"‚úì Feature Engineering Complete. Total Columns: {len(df_clean.columns)}")

"""### STEP 3.5: BALANCING DATASET"""

print("‚úì Balancing Dataset (Undersampling middle range)...")

low = df_clean[df_clean['Ethanol concentration'] < 0.70]
med_low = df_clean[(df_clean['Ethanol concentration'] >= 0.70) &
                   (df_clean['Ethanol concentration'] < 0.75)]
middle = df_clean[(df_clean['Ethanol concentration'] >= 0.75) &
                  (df_clean['Ethanol concentration'] < 0.85)]
high = df_clean[df_clean['Ethanol concentration'] >= 0.85]

# Sample only 40% of the dominant "Middle" class
middle_sampled = middle.sample(frac=0.40, random_state=42)

# Recombine and Shuffle
df_clean = pd.concat([low, med_low, middle_sampled, high], ignore_index=True)
df_clean = df_clean.sample(frac=1, random_state=42).reset_index(drop=True)

print(f"  New Dataset Size: {len(df_clean)} rows")

print("\nValue distribution:")
print(f"< 0.70: {(df_clean['Ethanol concentration'] < 0.70).sum()} samples")
print(f"0.70-0.75: {((df_clean['Ethanol concentration'] >= 0.70) & (df_clean['Ethanol concentration'] < 0.75)).sum()} samples")
print(f"0.75-0.85: {((df_clean['Ethanol concentration'] >= 0.75) & (df_clean['Ethanol concentration'] < 0.85)).sum()} samples")
print(f"0.85-0.95: {((df_clean['Ethanol concentration'] >= 0.85) & (df_clean['Ethanol concentration'] < 0.95)).sum()} samples")
print(f"> 0.95: {(df_clean['Ethanol concentration'] > 0.95).sum()} samples")

"""### STEP 4: DATA PREPARATION"""

# 1. Drop redundant features
# Note: We keep 'Temp_Bottom' (which came from T14) because it's controllable (Reboiler).
# We drop 'T14' raw column to avoid duplication.
features_to_drop = [
    'Ethanol concentration',  # Target
    'T1',                     # Dropping Top Temp as requested
    'Temp_Top',               # Redundant with T1
    'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14',
    # Drop intermediate calc columns if they exist
    'Temp_Diff_T1_T2', 'Temp_Diff_T2_T3', 'Temp_Diff_T3_T4', 'Temp_Diff_Global'
]

# 2. Select Features (X) and Target (y)
existing_cols_to_drop = [col for col in features_to_drop if col in df_clean.columns]
X = df_clean.drop(columns=existing_cols_to_drop)
y = df_clean['Ethanol concentration']

# 3. Validation
print(f"‚úì Final Feature Set: {X.shape[1]} features")
print("\nFinal Features List:")
for i, col in enumerate(X.columns, 1):
    print(f"  {i}. {col}")

"""### STEP 5: TRAIN-TEST-VALIDATION SPLIT"""

# 1. First split: train+val (80%) and test (20%)
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 2. Second split: train (60%) and validation (20%) from train+val
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.25, random_state=42 # 0.25 of 80% is 20% total
)

# 3. Feature scaling
scaler = StandardScaler()

# Fit ONLY on training data
X_train_scaled = scaler.fit_transform(X_train)

# Transform Val and Test using Train statistics
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print(f"\nData split:")
print(f"  Training: {X_train.shape[0]} samples (60%)")
print(f"  Validation: {X_val.shape[0]} samples (20%)")
print(f"  Testing: {X_test.shape[0]} samples (20%)")

"""### STEP 6: CROSS-VALIDATION & TRAINING MODELS"""

# Define K-Fold (5 Splits)
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
print("   ‚úì Using 5-Fold Cross-Validation")

# --- MODEL A: XGBoost (Regularized) ---
xgb_model = XGBRegressor(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.03,
    subsample=0.85,
    colsample_bytree=0.85,
    reg_alpha=0.1,
    reg_lambda=1.0,
    random_state=42,
    n_jobs=-1
)

# 1. Run Cross-Validation
print("   ... Validating XGBoost (this takes a moment)...")
xgb_cv_scores = cross_val_score(xgb_model, X_train_scaled, y_train, cv=kfold, scoring='r2')
print(f"   ‚úì XGBoost Mean CV R¬≤: {xgb_cv_scores.mean():.4f}")

# 2. Train Final Model
xgb_model.fit(X_train_scaled, y_train)
xgb_pred = xgb_model.predict(X_test_scaled)
xgb_r2 = r2_score(y_test, xgb_pred)
xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))
print(f"   ‚úì XGBoost Test R¬≤: {xgb_r2:.5f} | RMSE: {xgb_rmse:.5f}")


# --- MODEL B: Random Forest ---
rf_model = RandomForestRegressor(
    n_estimators=200,
    max_depth=20,
    min_samples_split=5,
    random_state=42,
    n_jobs=-1
)

# 1. Run Cross-Validation
print("   ... Validating Random Forest...")
rf_cv_scores = cross_val_score(rf_model, X_train_scaled, y_train, cv=kfold, scoring='r2')
print(f"   ‚úì Random Forest Mean CV R¬≤: {rf_cv_scores.mean():.4f}")

# 2. Train Final Model
rf_model.fit(X_train_scaled, y_train)
rf_pred = rf_model.predict(X_test_scaled)
rf_r2 = r2_score(y_test, rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
print(f"   ‚úì Random Forest Test R¬≤: {rf_r2:.5f} | RMSE: {rf_rmse:.5f}")

"""STEP 7: BEST MODEL SELECTION"""

print("\n‚è≥ STEP 9: Selecting Best Model & Saving...")

if xgb_r2 > rf_r2:
    best_model = xgb_model
    best_name = "XGBoost"
    best_r2 = xgb_r2
    best_rmse = xgb_rmse
else:
    best_model = rf_model
    best_name = "Random Forest"
    best_r2 = rf_r2
    best_rmse = rf_rmse

print(f"   üèÜ WINNER: {best_name}")

"""### STEP 8: FEATURE IMPORTANCE ANALYSIS"""

feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': best_model.feature_importances_
}).sort_values('Importance', ascending=False)

# Check Controllable Features
controllable_features = ['Reflux_Ratio', 'Reboiler_Intensity', 'Condenser_Load',
                         'Feed_Normalized', 'Column_Load']
controllable_in_top10 = feature_importance.head(10)[feature_importance.head(10)['Feature'].isin(controllable_features)]

print(f"   Top 5 Features: {list(feature_importance.head(5)['Feature'].values)}")

if len(controllable_in_top10) > 0:
    print(f"   ‚úÖ PHYSICS CHECK PASS: {len(controllable_in_top10)} controllable features in Top 10.")
else:
    print(f"   ‚ö†Ô∏è PHYSICS CHECK WARNING: Model relies mostly on temperatures.")

# 3. Plot (Now using the correct variable name)
plt.figure(figsize=(10, 6))
# Plot top 15 features
top_features = feature_importance.head(15)
plt.barh(top_features['Feature'][::-1], top_features['Importance'][::-1], color='#2196F3')
plt.title(f"Top 15 Features - {best_model}", fontsize=14, fontweight='bold')
plt.xlabel('Importance Score')
plt.tight_layout()
plt.savefig('feature_importance.png')
print("   ‚úì Plot saved to feature_importance.png")

"""### STEP 9: RESIDUAL ANALYSIS"""

import scipy.stats as stats

y_pred_final = best_model.predict(X_test_scaled)
residuals = y_test - y_pred_final

print(f"   Residual Mean: {residuals.mean():.6f} (Target ~0)")
print(f"   Residual Std:  {residuals.std():.6f}")

# Normality Check
_, p_value = stats.normaltest(residuals)
if p_value > 0.05:
    print(f"   ‚úÖ Residuals are normally distributed (p={p_value:.4f})")
else:
    print(f"   ‚ö†Ô∏è Residuals may not be normal (p={p_value:.4f})")

# 4-Way Residual Plot
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Predicted vs Actual
axes[0, 0].scatter(y_test, y_pred_final, alpha=0.6, s=20, color='steelblue')
axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
axes[0, 0].set_title(f'Predicted vs Actual (R¬≤ = {best_r2:.4f})', fontweight='bold')
axes[0, 0].grid(True, alpha=0.3)

# 2. Residuals vs Predicted
axes[0, 1].scatter(y_pred_final, residuals, alpha=0.6, s=20, color='steelblue')
axes[0, 1].axhline(y=0, color='r', linestyle='--', lw=2)
axes[0, 1].set_title('Residual Plot (Homoscedasticity)', fontweight='bold')
axes[0, 1].grid(True, alpha=0.3)

# 3. Histogram
axes[1, 0].hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='steelblue')
axes[1, 0].set_title('Distribution of Residuals', fontweight='bold')
axes[1, 0].grid(True, alpha=0.3)

# 4. Q-Q Plot
stats.probplot(residuals, dist="norm", plot=axes[1, 1])
axes[1, 1].set_title('Q-Q Plot', fontweight='bold')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('residual_analysis.png', dpi=300, bbox_inches='tight')
print(f"   ‚úì Saved: residual_analysis.png")

"""STEP 10: SAVE FILES"""

import pickle
print("\n‚è≥ Final Step: Best Model Saving...")

if xgb_r2 > rf_r2:
    best_model = xgb_model
    best_name = "XGBoost"
    best_r2 = xgb_r2
    best_rmse = xgb_rmse
else:
    best_model = rf_model
    best_name = "Random Forest"
    best_r2 = rf_r2
    best_rmse = rf_rmse

print(f"   üèÜ WINNER: {best_name}")

# SAVE FILES
with open('model.pkl', 'wb') as f:
    pickle.dump(best_model, f)

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

with open('features_names.pkl', 'wb') as f:
    pickle.dump(list(X.columns), f)

print("\n‚úÖ SUCCESS! The following files are ready for download:")
print("   1. model.pkl")
print("   2. scaler.pkl")
print("   3. features_names.pkl")
print("üëâ Place these in your Streamlit app folder.")

