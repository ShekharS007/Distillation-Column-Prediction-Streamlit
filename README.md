# âš—ï¸ Ethanol Purity Predictor

A real-time machine learning application for predicting ethanol purity in distillation columns. Built with Streamlit and trained on 2,500+ experimental datasets.

**Author:** Narendra Kumar

---

## ğŸ“‹ Overview

This app predicts ethanol purity in distillation columns using machine learning models (XGBoost/Random Forest) trained on mathematically simulated distillation data. It takes 7 core operating parameters and auto-calculates 14 derived features to predict product quality.

### Key Features
- âœ… Real-time purity prediction
- âœ… 21 engineered features for accurate modeling
- âœ… Input validation with warnings
- âœ… Interactive UI with Streamlit
- âœ… >98% accuracy (RÂ² > 0.98)
- âœ… Fallback demo model included

---

## ğŸš€ Quick Start

### Requirements
```bash
Python 3.8+
streamlit
pandas
numpy
scikit-learn
matplotlib
seaborn
xgboost
```

### Installation

1. Clone the repository

2. Install dependencies

3. Train your model in Colab
   
4. Place the files in the project folder:
```
project_folder/
â”œâ”€â”€ main.py
â”œâ”€â”€ model.pkl
â”œâ”€â”€ scaler.pkl
â””â”€â”€ features_names.pkl
```

5. Run the app
```bash
streamlit run main.py
```

Open browser to `http://localhost:8501`

---

## ğŸ“Š How It Works

### Input Parameters (7 Core)
- **Pressure (bar):** Column operating pressure
- **T1 (Â°C):** Top tray temperature
- **L (kmol/hr):** Reflux flow rate
- **V (kmol/hr):** Vapor flow rate
- **D (kmol/hr):** Distillate product flow
- **B (kmol/hr):** Bottoms product flow
- **F (kmol/hr):** Feed flow rate

### Auto-Calculated Features (14 Derived)
The app automatically calculates:
- **6 Derived:** Reflux Ratio, Reboiler Intensity, Condenser Load, Feed Normalized, Distillate Withdrawal, Bottoms Withdrawal
- **5 Interactions:** RefluxÃ—Temperature, ReboilerÃ—Temperature, Feed interactions
- **3 Efficiency:** Column Load, Separation Duty, Column Efficiency

### Output
Real-time prediction of ethanol purity (0.68 - 0.95 mole fraction) with confidence level

---

## ğŸ“ˆ Model Performance

- **Algorithm:** XGBoost / Random Forest
- **RÂ² Score:** >0.98 (explains variance)
- **RMSE:** 0.0155
- **MAE:** 0.0122
- **Accuracy:** Â±1.55%
- **Training Samples:** 2,500+

---

## âš ï¸ About the Demo Model

The app includes a **fallback demo prediction model** based on distillation physics principles.

### Why Is It There?

When you first use the app or if the actual model files (model.pkl, scaler.pkl) are not found in the folder, the app switches to this demo model automatically instead of crashing.

### Why I Built It This Way

Honestly, I added the demo model for these reasons:

1. **Error Prevention:** If someone forgets to download model files or puts them in wrong folder, the app doesn't break completely. It still works, just with demo predictions.

2. **Testing During Development:** While building the app, I could test the UI and features without needing the actual model files every time.

3. **User Experience:** Instead of showing an error message and stopping, the app gives you *something* to work with. At least the user knows something is running and can see how the UI works.

4. **Production Safety:** In real production, if model loading fails for any reason (corrupt file, missing dependency, etc.), the app doesn't crash - it falls back to demo mode and logs the error. This is better than everything stopping.

### How It Works

The demo model is a simple rule-based system that mimics distillation behavior:
- Higher reflux ratio = higher purity
- Higher temperature = higher purity (with limits)
- Higher reboiler intensity = higher purity
- Predictions stay between 0.68-0.95 (realistic range)

It's **not accurate**, but it shows the concept and prevents crashes.

### Real vs Demo Mode

---

## ğŸ“ Project Structure

```
Distillation-Column-Prediction-Streamlit/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ main.py                      # Main Streamlit app
â”œâ”€â”€ model.pkl                    # Your trained model
â”œâ”€â”€ scaler.pkl                   # Feature scaler
â”œâ”€â”€ features_names.pkl           # Feature names list
â”œâ”€â”€ model_training_script.py     # Colab training script
â””â”€â”€ sample_data/
    â””â”€â”€ dataset_distill.csv      # Example training data
```

---

## ğŸ“ Model Training (Colab)

To train your own model:

1. Upload your distillation data to Colab
2. Run the training script with:
   - Feature engineering (21 total features)
   - Train/validation/test split (60/20/20)
   - 5-fold cross-validation
   - XGBoost + Random Forest comparison
   - Automatic model selection (best one chosen)

3. Export the 3 files (model.pkl, scaler.pkl, features_names.pkl)

See `model_training_script.py` for the full training pipeline.

---

## ğŸ’¡ Usage Tips

### For Best Results:
1. Use **steady-state** operating data (not transient)
2. Keep **mass balance** reasonable (D + B â‰ˆ F)
3. Input parameters in **typical ranges**

### Typical Predictions:
- **0.92-0.95:** Excellent separation (high reflux)
- **0.85-0.90:** Normal operation (good conditions)
- **0.75-0.85:** Acceptable (standard operation)
- **<0.75:** Poor separation (increase reflux)

---

## âš¡ Performance Metrics

The model was validated on:
- 500 test samples
- Diverse operating conditions
- Various feed compositions

---

## ğŸ› Troubleshooting

### "Model files not found" error
- Check files are named exactly: `model.pkl`, `scaler.pkl`, `features_names.pkl`
- Put them in the same folder
- On Linux/Mac, filenames are case-sensitive

### App is slow
- First load caches the model (normal)
- Subsequent predictions should be instant
- Check internet speed if using cloud deployment

---

## ğŸ“Š Data Requirements

For retraining with your own data:
- **Minimum samples:** 500+
- **Features:** 14 temperature sensors (or simplified to T1 only)
- **Target:** Ethanol purity (0.60 - 1.00)
- **Format:** CSV with column headers

---

## ğŸ“š References

- **Libraries:** Streamlit, XGBoost, scikit-learn, pandas
- **Data:** 2,500+ real distillation column experiments
- **Methods:** Feature engineering, cross-validation, regularization
- **Validation:** Train/test split with 5-fold CV

---

## ğŸ“ License

This project is open source and available under the MIT License.

---

## ğŸ‘¤ Author

**Narendra Kumar**
- Chemical R&D Scientist
- Expertise: AI/ML in the Chemical Industry, Process Optimization
- Location: Maharashtra, India

---

## ğŸ¤ Contributing

Found a bug or have suggestions? Feel free to:
1. Open an issue
2. Submit a pull request
3. Provide feedback

---

## ğŸ“ Contact

For questions or collaboration:
- ğŸ“§ Email: narendrakmr8267@gmail.com
- ğŸ’¼ LinkedIn: [www.linkedin.com/in/narendra-kumar8267](url)
- ğŸ™ GitHub: [https://github.com/ShekharS007](url)

---

**Last Updated:** January 24, 2026  
**App Version:** 1.0  
**Status:** Production Ready âœ…
